---
title: "Computer Lab 3 - Unsupervised, semisupervised and active learning"
author: "Huong Nguyen and Tanetpong Choungprayoon"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

**INSTRUCTIONS**: 

- The sections named Intro do **not** have any problems for you. Those sections contain code used to set up the data and do some initial analysis, so just read and follow along by running each code chunk. 
- Your problems are clearly marked out as Problem 1, Problem 2 etc. You should answer all problems by adding code chunks and text below each question.
- Your submission in Athena should contain two files:
  - This Rmd file with your answers.
  - A PDF version of this file (use the knit to PDF option above).
- You can also write math expression via LaTeX, using the dollar sign, for exampleÂ´ $\beta$.
- You can navigate the sections of this file clicking (Top Level) in the bottom of RStudio's code window.

#### Loading packages and data

Loading some packages first. Do `install.packages()` for each package the first time you use a new package.

```{r loading-packages, echo=FALSE}
library(mvtnorm) #for constructing multivariate density
library(plyr) #converting factor into 0 1
library("RColorBrewer") # for pretty prettyColors
prettyColors = brewer.pal(12, "Paired")[c(1,2,7,8,3,4,5,6,9,10)];
options(repr.plot.width = 12, repr.plot.height = 12, repr.plot.res = 100) # plot size
set.seed(12332)         # set the seed for reproducability
```

```{r load-penguin-data, echo=FALSE}
penguins = read.csv("https://github.com/mattiasvillani/MLcourse/raw/main/Data/PalmerPenguins.csv")
xmin = min(penguins[,"flipper_length_cm"])
xmax = max(penguins[,"flipper_length_cm"])
ymin = min(penguins[,"body_mass_kg"])
ymax = max(penguins[,"body_mass_kg"])
plot(penguins[penguins[,"species"]=="Adelie","flipper_length_cm"], 
     penguins[penguins[,"species"]=="Adelie","body_mass_kg"], 
     col = prettyColors[2], xlim = c(xmin,xmax), ylim = c(ymin,ymax), pch = 19,
     xlab = "flipper length (cm)", ylab = "body mass (kg)")  
points(penguins[penguins[,"species"]=="Gentoo","flipper_length_cm"], 
       penguins[penguins[,"species"]=="Gentoo","body_mass_kg"], col = prettyColors[4], pch = 19)   
points(penguins[penguins[,"species"]=="Chinstrap","flipper_length_cm"], 
       penguins[penguins[,"species"]=="Chinstrap","body_mass_kg"], col = prettyColors[8], pch = 19)   
```

#### Problem 1 - Supervised GMM - LDA and QDA

Analyze the Penguin data using supervised Linear Discriminant Analysis (LDA) and 
Quadratic Discriminant Analysis (QDA). Write your own code. 
The end result of your analysis should included a figure similar to Figure 10.4 in the MLES book.
Are the assumptions in LDA plausible for this dataset?
[Hint: you can use the mvtnorm package to get the multivariate normal density].

\emph{We first recoded the categorical variable `species (m)` into numerical variable and then calculated $\hat{\mu}$,$\hat{\Sigma}$ and $\hat{\pi}$ for each `m`. We started from Quadratic Discriminant Analysis (QDA) assuming $\hat{\Sigma}$ is different across category. }

```{r Problem 1 - Supervised QDA GMM Learning Gaussian}
#coding based on L9 slide 7 and sml book page 207
  penguins$m = as.numeric(penguins$species)
#compute mu_m and sigma_m and phat_m
  penguins_1=subset(penguins, m == 1)
  penguins_2=subset(penguins, m == 2)
  penguins_3=subset(penguins, m == 3)
  #compute mu_m, n_m, p_m note that mu matrix is 1 by 2 matrix
  N = dim(penguins)[1]
  n_1 = dim(penguins_1)[1]
  mu_1 = as.matrix(colMeans(as.matrix(penguins_1[,2:3])))
  p_1 = n_1/N 
  n_2 = dim(penguins_2)[1]
  mu_2 = as.matrix(colMeans(as.matrix(penguins_2[,2:3])))
  p_2 = n_2/N 
  n_3 = dim(penguins_3)[1]
  mu_3 = as.matrix(colMeans(as.matrix(penguins_3[,2:3])))
  p_3 = n_3/N 
  #Compute sigma
    #sigma_1
  sigma_temp = matrix(0,nrow=2,ncol=2)
  for (i in 1:n_1) {
    sigma_i = t(as.matrix(penguins_1[i,2:3]-mu_1))%*%as.matrix(penguins_1[i,2:3]-mu_1)
    sigma_temp = sigma_temp+sigma_i
  }
    sigma_final_1 = sigma_temp/n_1
    #sigma_2
      sigma_temp = matrix(0,nrow=2,ncol=2)
  for (i in 1:n_2) {
    sigma_i = t(as.matrix(penguins_2[i,2:3]-mu_2))%*%as.matrix(penguins_2[i,2:3]-mu_2)
    sigma_temp = sigma_temp+sigma_i
  }
    sigma_final_2 = sigma_temp/n_2
    #sigma_3
    sigma_temp = matrix(0,nrow=2,ncol=2)
  for (i in 1:n_3) {
    sigma_i = t(as.matrix(penguins_3[i,2:3]-mu_3))%*%as.matrix(penguins_3[i,2:3]-mu_3)
    sigma_temp = sigma_temp+sigma_i
  }
    sigma_final_3 = sigma_temp/n_3
```

\emph{In order to plot decision boundaries for classification, we created arbitrary X-Grid and then predict the penguin species for each combination of `body mass` and `flipper length`. To get the predicted classification, we computed $\delta$ from the sum of logarithm of $\hat{\pi}$ and multivariate normal density with $\hat{\mu}$ and $\hat{\Sigma}$ for each category. Then we chose the category that giving highest value of $\delta$ as our predicted classification.}

```{r Problem 1 - Supervised Predict QDA GMM Gaussian}
#Create x_grid based on the actual scale, 2:8 for body mass 15:25 for flipper length
body_mass_grid = seq(min(penguins[,"body_mass_kg"]),max(penguins[,"body_mass_kg"]), length = 50)
flipper_length_grid = seq(min(penguins[,"flipper_length_cm"]),max(penguins[,"flipper_length_cm"]), length = 50)
      x_grid = expand.grid(body_mass_grid,flipper_length_grid) #combination of these two grids
      #data = (as.matrix(penguins[,2:3]))
      
#compute delta_m
  delta_1 <- log(p_1) + dmvnorm(x_grid, mu_1, sigma_final_1, log=TRUE) 
  delta_2 <- log(p_2) + dmvnorm(x_grid, mu_2, sigma_final_2, log=TRUE)
  delta_3 <- log(p_3) + dmvnorm(x_grid, mu_3, sigma_final_3, log=TRUE)
  predict_QDA <- cbind(x_grid,delta_1,delta_2,delta_3) 
  #select the y from the group with highest value of delta
  predict_QDA$y <- colnames(predict_QDA[,3:5])[apply(predict_QDA[,3:5],1,which.max)] 
  #rename for plotting
  names(predict_QDA)[1] <- "body_mass_kg"
  names(predict_QDA)[2] <- "flipper_length_cm"
```

\emph{We plotted our QDA predicted value from our arbitrary X-grid against actual points.}

```{r Problem 1 - Plot QDA Decision Boundary, echo=FALSE}
plot(penguins[penguins[,"species"]=="Adelie","flipper_length_cm"], 
     penguins[penguins[,"species"]=="Adelie","body_mass_kg"], 
     col = prettyColors[2], xlim = c(xmin,xmax), ylim = c(ymin,ymax), pch = 19,
     xlab = "flipper length (cm)", ylab = "body mass (kg)")  
points(predict_QDA[predict_QDA[,"y"]=="delta_1","flipper_length_cm"], 
       predict_QDA[predict_QDA[,"y"]=="delta_1","body_mass_kg"], col = prettyColors[2], pch = 4)
points(penguins[penguins[,"species"]=="Gentoo","flipper_length_cm"], 
       penguins[penguins[,"species"]=="Gentoo","body_mass_kg"], col = prettyColors[4], pch = 19) 
points(predict_QDA[predict_QDA[,"y"]=="delta_3","flipper_length_cm"], 
       predict_QDA[predict_QDA[,"y"]=="delta_3","body_mass_kg"], col = prettyColors[4], pch = 4)
points(penguins[penguins[,"species"]=="Chinstrap","flipper_length_cm"], 
       penguins[penguins[,"species"]=="Chinstrap","body_mass_kg"], col = prettyColors[8], pch = 19)
points(predict_QDA[predict_QDA[,"y"]=="delta_2","flipper_length_cm"], 
       predict_QDA[predict_QDA[,"y"]=="delta_2","body_mass_kg"], col = prettyColors[8], pch = 4)
```

\emph{For Linear Discriminant Analysis (LDA), we recalculated $\hat{\Sigma}$.}
```{r Problem 1 - Supervised LDA GMM Learning Gaussian}
#Recalculate only on sigma (we just sum all group and divided by N)
  #sumofsigma 1
  sigma_temp = matrix(0,nrow=2,ncol=2)
  for (i in 1:n_1) {
    sigma_i = t(as.matrix(penguins_1[i,2:3]-mu_1))%*%as.matrix(penguins_1[i,2:3]-mu_1)
    sigma_temp = sigma_temp+sigma_i
  }
    sigma_lda_1 = sigma_temp
  #sumofsigma 2
      sigma_temp = matrix(0,nrow=2,ncol=2)
  for (i in 1:n_2) {
    sigma_i = t(as.matrix(penguins_2[i,2:3]-mu_2))%*%as.matrix(penguins_2[i,2:3]-mu_2)
    sigma_temp = sigma_temp+sigma_i
  }
    sigma_lda_2 = sigma_temp
  #sumofsigma 3
    sigma_temp = matrix(0,nrow=2,ncol=2)
  for (i in 1:n_3) {
    sigma_i = t(as.matrix(penguins_3[i,2:3]-mu_3))%*%as.matrix(penguins_3[i,2:3]-mu_3)
    sigma_temp = sigma_temp+sigma_i
  }
    sigma_lda_3 = sigma_temp
#average sigma lda
    sigma_lda = (sigma_lda_1+sigma_lda_2+sigma_lda_3)/N
```

\emph{For LDA, we computed $\delta$ from the sum of logarithm of $\hat{\pi}$ and multivariate normal density with $\hat{\mu}$ for each category and the same $\hat{\Sigma}$ for each categories. Then we chose the category that giving highest value of $\delta$ as our predicted classification.}
```{r Problem 1 - Supervised Predict LDA GMM Gaussian}
  delta_1 <- log(p_1) + dmvnorm(x_grid, mu_1, sigma_lda, log=TRUE) #do we have to multiplied by -1?
  delta_2 <- log(p_2) + dmvnorm(x_grid, mu_2, sigma_lda, log=TRUE)
  delta_3 <- log(p_3) + dmvnorm(x_grid, mu_3, sigma_lda, log=TRUE)
  predict_LDA <- cbind(x_grid,delta_1,delta_2,delta_3) 
  #select the y from the group with highest delta
  predict_LDA$y <- colnames(predict_LDA[,3:5])[apply(predict_LDA[,3:5],1,which.max)] 
  #rename for plotting
  names(predict_LDA)[1] <- "body_mass_kg"
  names(predict_LDA)[2] <- "flipper_length_cm"

```

\emph{We plotted our LDA predicted value from our arbitrary X-grid against actual points.}

```{r Problem 1 - Plot LDA Decision Boundary, echo=FALSE}
plot(penguins[penguins[,"species"]=="Adelie","flipper_length_cm"], 
     penguins[penguins[,"species"]=="Adelie","body_mass_kg"], 
     col = prettyColors[2], xlim = c(xmin,xmax), ylim = c(ymin,ymax), pch = 19,
     xlab = "flipper length (cm)", ylab = "body mass (kg)")  
points(predict_LDA[predict_LDA[,"y"]=="delta_1","flipper_length_cm"], 
       predict_LDA[predict_LDA[,"y"]=="delta_1","body_mass_kg"], col = prettyColors[2], pch = 4)
points(penguins[penguins[,"species"]=="Gentoo","flipper_length_cm"], 
       penguins[penguins[,"species"]=="Gentoo","body_mass_kg"], col = prettyColors[4], pch = 19) 
points(predict_LDA[predict_LDA[,"y"]=="delta_3","flipper_length_cm"], 
       predict_LDA[predict_LDA[,"y"]=="delta_3","body_mass_kg"], col = prettyColors[4], pch = 4)
points(penguins[penguins[,"species"]=="Chinstrap","flipper_length_cm"], 
       penguins[penguins[,"species"]=="Chinstrap","body_mass_kg"], col = prettyColors[8], pch = 19)
points(predict_LDA[predict_LDA[,"y"]=="delta_2","flipper_length_cm"], 
       predict_LDA[predict_LDA[,"y"]=="delta_2","body_mass_kg"], col = prettyColors[8], pch = 4)
```

\emph{Given the fact the the label is known, we think that the QDA is more appropriate. In other words, the assumptions in LDA do not seem plausible for this dataset. Looking at the data alone, we can simply infer that the covariance matrix of each class is different. Moreover, it seems that two classes illustrated with blue and red colors are more related to one another while the orange color coded class seems to be distinct.}

\newpage
#### Problem 2 - Unsupervised GMM
Pretend now that the labels of the Penguins are unknown. Use the EM for multivariate GMM code
on the course web page (under Lecture 9)
[GMM_EM_Multi.R](https://github.com/mattiasvillani/MLcourse/raw/main/Code/GMM_EM_Multi.R). 
Use the code to fit a Gaussian mixture model to the penguin data for M=1, 2 and 3 mixture components.
Set reasonable initial values for the EM algorithm (at least take into account the scale of the data).


*We used the ``GMM_EM_Multi.R`` from the course web to estimate our parameters of interest. We constructed our initial parameters according to the set up required by ``GMM_EM_Multi.R``. We started fitting a Gaussian mixture model to the penguin data for M=1. Note that we made used of available information by simply setting initial mean and variance to the whole sample mean and variance.*
```{r Problem 2 Define function}
#Use the code from the course web page
mixtureMultiGaussianEM <- function(data, M, initMu, initSigma, initPi, tol){
  # Preliminaries
  count <- 0
  n <- dim(data)[1]
  nHat <- rep(0,M)
  W = matrix(0,n,M)  # n x m matrix with weights for all observations and all components.
  Mu = initMu        
  Sigma = initSigma
  Pi = initPi
  unitVect = rep(1,n) # Just a vector of ones that we need later for efficiency
  
  LogLOld <- 10^10
  LogLDiff <- 10^10
  while (LogLDiff > tol){
    count <- count + 1
    
    # E-step
    
    for (m in 1:M){
      W[,m] = Pi[m]*dmvnorm(data, Mu[,m], Sigma[,,m])
    }
    sum_w <- rowSums(W)
    for (m in 1:M){
      W[,m] = W[,m]/sum_w
    }
    
    # M-step
    for (m in 1:M){
      nHat[m] <- sum(W[,m])
      Mu[,m] = (1/nHat[m])*crossprod(W[,m],data)
      Res = data - tcrossprod(unitVect,Mu[,m])
      Sigma[,,m] = (1/nHat[m])*t(Res)%*%diag(W[,m])%*%Res # Matrix version of the estimate in the slides
      Pi[m] = nHat[m]/n
    }
    
    # Log-Likelihood computation - to check convergence
    for (m in 1:M){
      W[,m] = Pi[m]*dmvnorm(data, Mu[,m], Sigma[,,m])
    }
    LogL = sum(log(rowSums(W)))
    LogLDiff = abs(LogL - LogLOld)
    LogLOld = LogL
    
  }
  return(list(Mu = Mu, Sigma = Sigma, Pi = Pi, LogL = LogL, nIter = count))
}

```


*We used ``GMM_EM_Multi.R`` to fit a Gaussian mixture model to the penguin data for M=1. Note that we made used of available information by simply setting initial mean and variance to the whole sample mean and variance.*
```{r setting values for EM function for M is 1}
# p (=2) x M (1) matrix
#data is a n x p matrix with n observations on p variables
data <- as.matrix(penguins[,2:3]) #without changing data, it got stucks real bad
M_1 = 1
# initMu is an p x M matrix with initial values for the component means
initMu_1 = as.matrix(rbind(mean(penguins$body_mass_kg),mean(penguins$flipper_length_cm)))
# initSigma is an p x p x M 3D array with initial values for the component covariance matrices
initSigma_1 = array(0,c(2,2,1))
initSigma_1[1,1,] = (sd(penguins$body_mass_kg))
initSigma_1[2,2,] = (sd(penguins$flipper_length_cm))
#initPi is a M-dim vector with initial values for the component probabilities
initPi_1 = c(1)

EMfit_1 = mixtureMultiGaussianEM(data, M_1, initMu_1, initSigma_1, initPi_1, tol = 0.0000001)
```

*We used `GMM_EM_Multi.R` to fit a Gaussian mixture model to the penguin data for M=2. Note that we made used of available information by simply setting initial means of each class from random sampling from whole samples mean and variance to the whole variance.*
```{r setting values for EM function for M is 2}
M_2 = 2
# initMu is an p x M matrix with initial values for the component means
initMu_2 = matrix(NA,2,2)
initMu_2[1,] = rnorm(2, mean = mean(penguins$body_mass_kg), sd=sd(penguins$body_mass_kg))
initMu_2[2,] = rnorm(2, mean = mean(penguins$flipper_length_cm), sd=sd(penguins$flipper_length_cm))
# initSigma is an p x p x M 3D array with initial values for the component covariance matrices
initSigma_2 = array(0,c(2,2,2))
initSigma_2[,,1] = (sd(penguins$body_mass_kg))*diag(2)
initSigma_2[,,2] = (sd(penguins$flipper_length_cm))*diag(2)
#initPi is a M-dim vector with initial values for the component probabilities
initPi_2 = c(0.5,0.5)

EMfit_2 = mixtureMultiGaussianEM(data, M_2, initMu_2, initSigma_2, initPi_2, tol = 0.0000001)
```

*We used `GMM_EM_Multi.R` to fit a Gaussian mixture model to the penguin data for M=3. Note that we made used of available information by simply setting initial means of each class from random sampling from whole samples mean and variance to the whole variance and identical matrix.*
```{r setting values for EM function for M is 3}
M_3 = 3
# initMu is an p x M matrix with initial values for the component means
initMu_3 = matrix(NA,2,3)
initMu_3[1,] = rnorm(3, mean = mean(penguins$body_mass_kg), sd=sd(penguins$body_mass_kg))
initMu_3[2,] = rnorm(3, mean = mean(penguins$flipper_length_cm), sd=sd(penguins$flipper_length_cm))
# initSigma is an p x p x M 3D array with initial values for the component covariance matrices
initSigma_3 = array(0,c(2,2,3))
initSigma_3[,,1] = (sd(penguins$body_mass_kg))*diag(2)
initSigma_3[,,2] = (sd(penguins$flipper_length_cm))*diag(2)
initSigma_3[,,3] = 1*diag(2) #????????
#initSigma_3[,,3]?
#initPi is a M-dim vector with initial values for the component probabilities
initPi_3 = c(1/3,1/3,1/3)

EMfit_3 = mixtureMultiGaussianEM(data, M_3, initMu_3, initSigma_3, initPi_3, tol = 0.0000001)
```

\emph{We plotted our estimated mean when  M=1 (green color coded), M=2 (red color coded) and M=3(blue color coded) against actual data.}
```{r Plot three means}
plot(penguins[penguins[,"species"]=="Adelie","flipper_length_cm"], 
     penguins[penguins[,"species"]=="Adelie","body_mass_kg"], 
     col = prettyColors[1], xlim = c(xmin,xmax), ylim = c(ymin,ymax), pch = 4,
     xlab = "flipper length (cm)", ylab = "body mass (kg)")
points(penguins[penguins[,"species"]=="Gentoo","flipper_length_cm"], 
       penguins[penguins[,"species"]=="Gentoo","body_mass_kg"], col = prettyColors[3], pch = 4) 
points(penguins[penguins[,"species"]=="Chinstrap","flipper_length_cm"], 
       penguins[penguins[,"species"]=="Chinstrap","body_mass_kg"], col = prettyColors[9], pch = 4)
#add 1 cluster mean point
points(EMfit_1$Mu[2,],EMfit_1$Mu[1,], col = prettyColors[6], pch = 15)
#add 2 cluster mean points showing red
points(EMfit_2$Mu[2,1],EMfit_2$Mu[1,1], col = prettyColors[8], pch = 15)
points(EMfit_2$Mu[2,2],EMfit_2$Mu[1,2], col = prettyColors[8], pch = 15)
#add 3 cluster mean points showing blue
points(EMfit_3$Mu[2,1],EMfit_3$Mu[1,1], col = prettyColors[2], pch = 15)
points(EMfit_3$Mu[2,2],EMfit_3$Mu[1,2], col = prettyColors[2], pch = 15)
points(EMfit_3$Mu[2,3],EMfit_3$Mu[1,3], col = prettyColors[2], pch = 15)
```

\newpage
#### Problem 3 - Semi-supervised GMM
Pretend now that the label for every odd observation in the dataset is known,
but the label for every even observation is unknown. Modify the
[GMM_EM.R](https://github.com/mattiasvillani/MLcourse/raw/main/Code/GMM_EM_Multi.R) code to semi-supervised GMM; the function mixtureMultiGaussianEM should have an additional argument which contains a vector of labels (NA for unknown labels). Analyze the penguin data using a semi-supervised 
Gaussian mixture model with three mixture components.

*We modified the `GMM_EM_Multi.R` from the course web by adding the function that reassigns the value of $w$ according to the label. We followed the course/book method by assigning 1 to $w_{i}(m)$ to or 0 (1 if $y_{i}=m$) during the E-step.*

```{r}
#We add addtional argument in the E-Step
onehot <- function(x){
    levels = sort(unique(x))
    onehotMatrix = matrix(0, length(x), length(levels))
    count = 0
    for (level in levels){
        count = count + 1
        onehotMatrix[x == level, count] = 1
    }
    return(onehotMatrix)
} 

mixtureMultiGaussianEM_semi <- function(data, M, initMu, initSigma, initPi, tol){
  # Preliminaries
  count <- 0
  n <- dim(data)[1]
  nHat <- rep(0,M)
  W = matrix(27,n,M)  # n x m matrix with weights for all observations and all components.
  Mu = initMu        
  Sigma = initSigma
  Pi = initPi
  unitVect = rep(1,n) # Just a vector of ones that we need later for efficiency
  
  LogLOld <- 10^10
  LogLDiff <- 10^10
  while (LogLDiff > tol){
    count <- count + 1
    
    # E-step
    for (m in 1:M) W[,m] = Pi[m]*dmvnorm(data, Mu[,m], Sigma[,,m])
    for (j in seq(1, n, 2)) W[j,] = onehot(penguins$species)[j,]
    sum_w <- rowSums(W)
    for (m in 1:M){
      W[,m] = W[,m]/sum_w
    }
    
    # M-step
    for (m in 1:M){
      nHat[m] <- sum(W[,m])
      Mu[,m] = (1/nHat[m])*crossprod(W[,m],data)
      Res = data - tcrossprod(unitVect,Mu[,m])
      Sigma[,,m] = (1/nHat[m])*t(Res)%*%diag(W[,m])%*%Res
      # Matrix version of the estimate in the slides
      Pi[m] = nHat[m]/n
    }
    
    # Log-Likelihood computation - to check convergence
    for (m in 1:M) W[,m] = Pi[m]*dmvnorm(data, Mu[,m], Sigma[,,m])
    for (j in seq(1, n, 2)) W[j,] = onehot(penguins$species)[j,]
    LogL = sum(log(rowSums(W)))
    LogLDiff = abs(LogL - LogLOld)
    LogLOld = LogL
    
  }
  return(list(Mu = Mu, Sigma = Sigma, Pi = Pi, LogL = LogL,
              nIter = count, W = head(W)))
}

```

*We set the label for every even observation is unknown.Then we used our modified `GMM_EM_Multi.R` to fit a semi-supervised Gaussian mixture model to the penguin data for M=3. Note that we made used of available information by simply setting initial means of each class from random sampling from whole samples mean and variance to the whole variance and identical matrix.*

```{r}
#Make even obs unknown
penguins_semi <- penguins
penguins_semi$row_odd <- seq_len(nrow(penguins_semi)) %% 2 
penguins_semi$species[penguins_semi$row_odd == "0"] <- NA
```

```{r}
data1 <- as.matrix(penguins_semi[,2:3])
M_3 = 3
# initMu is an p x M matrix with initial values for the component means
initMup_3 = matrix(NA,2,3)
initMu_3[1,] = rnorm(3, mean = mean(penguins$body_mass_kg), sd=sd(penguins$body_mass_kg))
initMu_3[2,] = rnorm(3, mean = mean(penguins$flipper_length_cm), sd=sd(penguins$flipper_length_cm))
# initSigma is an p x p x M 3D array with initial values for the component covariance matrices
initSigma_3 = array(0,c(2,2,3))
initSigma_3[,,1] = (sd(penguins$body_mass_kg))*diag(2)
initSigma_3[,,2] = (sd(penguins$flipper_length_cm))*diag(2)
initSigma_3[,,3] = 0.5*diag(2) #????????
#initSigma_3[,,3]?
#initPi is a M-dim vector with initial values for the component probabilities
initPi_3 = c(1/3,1/3,1/3)

EMfit_p3 = mixtureMultiGaussianEM_semi(data, M_3, initMu_3, initSigma_3, initPi_3, tol = 0.0000001)

```
\emph{We compared the result by plotting our estimated mean when  M=3 for unsupervised cases (red color coded) and supervised cases (blue color coded) against actual data.}

```{r prob3 compared three means}
plot(penguins[penguins[,"species"]=="Adelie","flipper_length_cm"], 
     penguins[penguins[,"species"]=="Adelie","body_mass_kg"], 
     col = prettyColors[1], xlim = c(xmin,xmax), ylim = c(ymin,ymax), pch = 4,
     xlab = "flipper length (cm)", ylab = "body mass (kg)")
points(penguins[penguins[,"species"]=="Gentoo","flipper_length_cm"], 
       penguins[penguins[,"species"]=="Gentoo","body_mass_kg"], col = prettyColors[3], pch = 4) 
points(penguins[penguins[,"species"]=="Chinstrap","flipper_length_cm"], 
       penguins[penguins[,"species"]=="Chinstrap","body_mass_kg"], col = prettyColors[9], pch = 4)
#add unsupervised mean points showing red
points(EMfit_p3$Mu[2,1],EMfit_p3$Mu[1,1], col = prettyColors[8], pch = 15)
points(EMfit_p3$Mu[2,2],EMfit_p3$Mu[1,2], col = prettyColors[8], pch = 15)
points(EMfit_p3$Mu[2,3],EMfit_p3$Mu[1,3], col = prettyColors[8], pch = 15)
#add supervised mean points showing blue
points(EMfit_3$Mu[2,1],EMfit_3$Mu[1,1], col = prettyColors[2], pch = 15)
points(EMfit_3$Mu[2,2],EMfit_3$Mu[1,2], col = prettyColors[2], pch = 15)
points(EMfit_3$Mu[2,3],EMfit_3$Mu[1,3], col = prettyColors[2], pch = 15)
```

\newpage
#### Problem 4 - Active learning - logistic regression
Use the Penguin data with the two species "Adelie" and "Chinstrap", only. Pretend then for the beginning that the species (labels) are not known, but keep this information such that the oracle can label queried data points later. 
Choose randomly 15 data points and label them. Run then active learning to query additional 45 data points based on a logistic regression model. Use both uncertainty sampling and variance reduction with an E-optimal design. Plot the labeled dataset after 60 labeled observations and compare between uncertainty sampling and E-optimality. Report the parameter estimates or the decision boundaries.

*We created a dataset with two species "Adelie" and "Chinstrap".*
```{r Data prep}
penguins_p4 = subset(penguins, species == "Adelie" | species == "Chinstrap" )
```

*We used the ``active_learning_UD.r`` from the course web to estimate our parameters of interest by uncertainty sampling method. We chose randomly 15 data points and label them and ran active learning to query additional 45 data points based on a logistic regression model. We plotted the labeled dataset after 60 labeled observations*
```{r active learning Uncertainty Sampling}
#Follow the step of active learning from course webpage
data1 <- as.matrix(penguins_p4[,2:3])
# choose query strategy for active learning: uncertainty sampling ("U") or D-optimality ("D")
#Start with U as it's easier to interpret
querys <- "U"
# size of unlabeled data pool
n      <-  dim(data)[1]
# first part until init uses random sampling
init   <- 15  
# final number to be labeled
end    <- 60
# (y-data, group variable not known to the learner)
group <- as.factor(as.numeric(penguins_p4$species))
group <-  revalue(group, c("1"=0))
group <-  revalue(group, c("2"=1))
group <-  as.numeric(as.character(group)) #Make sure that we have 0,1 value as example

# label randomly for initialisation (lab variable known to the learner)
lab    <- rep(NA, n)
rindex <- sample(n, init)
lab[rindex] <- group[rindex]

#Plot with unlabeled data and initial random labeling
plot(data, col=1, xlab=expression(x[1]), ylab=expression(x[2]))
points(data, col=4-2*lab, lwd=2, pch=16)

# initialize vector for saving accuracy development
accdev <- NULL
# X0 is design matrix for all x-data; first a column with 1's for the intercept, then the two features
X0 <- cbind(rep(1, n), data)

# sequential labeling
for (i in (init+1):end){ 
  # Maximum Likelihood Estimate in logistic regression 
  lgm   <- glm(lab ~ data[, 1] + data[, 2], family="binomial")
  beta  <- summary(lgm)$coef[, 1]

  # compute predictions for all data points
  predic  <- 1/(1+exp(-beta[1]-beta[2]*data[,1]-beta[3]*data[,2]))
  # compute accuracy
  accura  <- 1-mean(abs((predic>0.5)-group))
  accdev  <- cbind(accdev, c(i-1, accura))
  if (querys=="U"){
    uncert  <- abs(predic-0.5)
    uncert[!is.na(lab)] <- NA
    # determine index of data point to be queried
    index   <- which.min(uncert)
  }
  # oracle labels the queried data point
  lab[index] <- group[index]
  points(t(data[index, ]), col=4-2*lab[index], lwd=2, pch=19)
  #summarize stat
  accuracy_U = accura
  beta_U = beta
}
```
*We modified the ``active_learning_UD.r`` from the course web to estimate our parameters of interest by variance reduction with an E-optimal design. Instead of minimizing determinant as specified in the original code, we minimize the eigenvalues. We simply obtained the minimum value of eigenvalue of the information matrix and added E-criterion accordingly. Like previous method, we chose randomly 15 data points and label them and ran active learning to query additional 45 data points based on a logistic regression model. We plotted the labeled dataset after 60 labeled observations*
```{r active learning E optimality}

#For E we need to minimize the eigenvalue instead of minize det
querys <- "E"

# label randomly for initialisation (lab variable known to the learner)
lab    <- rep(NA, n)
rindex <- sample(n, init)
lab[rindex] <- group[rindex]

#Plot with unlabeled data and initial random labeling
plot(data, col=1, xlab=expression(x[1]), ylab=expression(x[2]))
points(data, col=4-2*lab, lwd=2, pch=16)

# initialize vector for saving accuracy development
accdev <- NULL
# X0 is design matrix for all x-data; first a column with 1's for the intercept, then the two features
X0 <- cbind(rep(1, n), data)

# sequential labeling
for (i in (init+1):end){ 
  # Maximum Likelihood Estimate in logistic regression 
  lgm   <- glm(lab ~ data[, 1] + data[, 2], family="binomial")
  beta  <- summary(lgm)$coef[, 1]

  # compute predictions for all data points
  predic  <- 1/(1+exp(-beta[1]-beta[2]*data[,1]-beta[3]*data[,2]))
  # compute accuracy
  accura  <- 1-mean(abs((predic>0.5)-group))
  accdev  <- cbind(accdev, c(i-1, accura))
  if (querys=="E"){
    # XL is design matrix for labeled data; VL is diagonal of W-matrix for labeled data
    XL   <- X0[!is.na(lab), ]
    G0   <- 1/(1+exp(-X0 %*% beta)) 
    V0   <- G0 * (1-G0)
    VL   <- V0[!is.na(lab)]
    crit <- rep(NA, n)
    for (j in 1:n){
      if (is.na(lab[j])){
        # X is design matrix XL plus an additional unlabeled data point j; V accordingly
        X    <- rbind(XL, X0[j, ])
        V    <- c(VL, V0[j])
        # Information matrix if x_j is added and E-criterion
        Infj <- t(X) %*% diag(V) %*% X
        minE <- min(eigen(Infj)$values)
        crit[j] <- 1/minE
      }
    }
    # determine index of data point to be queried
    index <- which.min(crit)
  }
  # oracle labels the queried data point
  lab[index] <- group[index]
  points(t(data[index, ]), col=4-2*lab[index], lwd=2, pch=19)
  #summarize stat
  accuracy_E = accura
  beta_E = beta
}

```
*The parameter estimates from these two method are reported below.*

```{r print result p4}
message(paste("Beta for U: ", format(beta_U, digits = 2)))
message(paste("Beta for E-optimality: ",format(beta_E, digits = 2)))
message(paste("Accuracy for Uncertainty Sampling: ",format(accuracy_U, digits = 4)))
message(paste("accuracy for E-optimality: ",format(accuracy_E, digits = 4)))
```
*It seems like variance reduction method can better cope with outliers in x-distribution. The labeled observarion are stick together in the uncertainty sampling while variance reduction is more scattered.*


