---
title: "Gibbs sampling, Metropolis-Hastings and Stan"
author: "Huong Nguyen and Tanetpong Choungprayoon"
output:
  pdf_document: default
---

```{r, warning=FALSE, message=FALSE, echo=FALSE}
  library(tidyverse)
  library(ggplot2)
  library(geoR)
  library(mvtnorm)
  library(RColorBrewer)
  library(MASS) #for mvrnorm in RMVNORM
  prettyCol = brewer.pal(10,"Paired")
  library(rstan)
```

\section{Question 1}
(a) \emph{Normal Model}

  i) We implement Gibbs sampler following these distributions:
$$\mu|\sigma^2,x \sim N(\mu_n, \tau_n^2) $$
$$\sigma^2|\mu,x \sim Inv - \chi^2 ({\nu_n}, \frac{\nu_0\sigma^2+\sum(x_i-\mu)^2}{n+\nu_0}) $$


* For simplicity we set up prior as follows: 
$\mu_0 = 0$
$\tau_0^2 = 10$
$\sigma_0^2 = 5$ and 
$\nu_0 = 2$
```{r, warning=FALSE, message=FALSE, echo=FALSE}
rainfall <- read.table("https://github.com/mattiasvillani/BayesLearnCourse/raw/master/Labs/rainfall.dat", 
                            header=FALSE)
rScaledInvChi2 <- function(n, df, scale){
      return((df*scale)/rchisq(n,df=df))
    }  
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
  #Setting
  X = as.matrix(rainfall)
  nComp <- 1    # Number of mixture components
  nIter <- 1000 # Number of Gibbs sampling draws
  nObs <- length(X)
  # Prior options
  muPrior <- 0 # Prior mean of mu
  tau2Prior <- 10 # Prior std of mu
  sigma2_0 <- 5
  nu0 <-2 # degrees of freedom for prior on sigma2
  # Initial value for the MCMC
  X = as.matrix(rainfall)
  mu <- quantile(X, probs = seq(0,1,length = nComp))
  sigma2 <- rep(var(X),nComp)
  probObsInComp <- rep(NA, nComp)
```

We wrote a code that simulates from the joint posteriors where $\mu,  \sigma^2$ are calculated following the formula in Lecture 7.


```{r, echo=FALSE}
set.seed(100121)
#Store Value
{
  gibbsDraws <- data.frame(matrix(0, ncol = 2 , nrow = nIter)) #storage
  #Start 
  sigma2 <- sigma2_0
  for(i in 1:1){
    precPrior <- 1/tau2Prior
    precData <- nObs/sigma2
    precPost <- precPrior + precData
    wPrior <- precPrior/precPost
    muPost <- wPrior*muPrior + (1-wPrior)*mean(X)
    tau2Post <- 1/precPost
    mu[i] <- rnorm(1, mean = muPost, sd = sqrt(tau2Post))
    sigma2[i] <- rScaledInvChi2(1, df = nu0 + nObs, scale = (nu0*sigma2_0 + sum((X - mu[i])^2))/(nu0 + nObs))
    #Plug in Data
    gibbsDraws[i,] = c(mu[i],sigma2[i])
  }
  for (j in 2:nIter){
    precPrior <- 1/tau2Prior
    precData <- nObs/sigma2
    precPost <- precPrior + precData
    wPrior <- precPrior/precPost
    muPost <- wPrior*mu[j-1] + (1-wPrior)*mean(X)
    tau2Post <- 1/precPost
    mu[j] <- rnorm(1, mean = muPost, sd = sqrt(tau2Post))
    sigma2[j] <- rScaledInvChi2(1, df = nu0 + nObs, scale = (nu0*sigma2_0 + sum((X - mu[j])^2))/(nu0 + nObs))
    #Plug in Data
    gibbsDraws[j,] = c(mu[j],sigma2[j])
  }
}
```


  ii) After 1,000 Iterations, we obtain posteriors illustrated as following plots.

```{r, echo=FALSE}
par(mfrow = c(2,1))
plot(gibbsDraws[,1], type = "l", xlab = "iteration",ylab = "Mu from Gibbs Sampling", col = prettyCol[2]);
plot(gibbsDraws[,2], type = "l", xlab = "iteration",ylab = "Sigma from Gibbs Sampling", col = prettyCol[8]);
par(mfcol=c(1,1))
```

It seems like the values of sampled parameters ($\mu,\sigma$) of each iteration are around the certain range (e.g. 31 - 33 for $\mu$). This reflects the convergence of these parameters.

To see if the iterated parameters actually converge, we run another 2 chains to make sure that they converge to the same distribution.
```{r, echo=FALSE}
set.seed(160121)
#Store Value
{
  gibbsDraws2 <- data.frame(matrix(0, ncol = 2 , nrow = nIter)) #storage
  #Start 
  sigma2 <- sigma2_0
  for(i in 1:1){
    precPrior <- 1/tau2Prior
    precData <- nObs/sigma2
    precPost <- precPrior + precData
    wPrior <- precPrior/precPost
    muPost <- wPrior*muPrior + (1-wPrior)*mean(X)
    tau2Post <- 1/precPost
    mu[i] <- rnorm(1, mean = muPost, sd = sqrt(tau2Post))
    sigma2[i] <- rScaledInvChi2(1, df = nu0 + nObs, scale = (nu0*sigma2_0 + sum((X - mu[i])^2))/(nu0 + nObs))
    #Plug in Data
    gibbsDraws2[i,] = c(mu[i],sigma2[i])
  }
  for (j in 2:nIter){
    precPrior <- 1/tau2Prior
    precData <- nObs/sigma2
    precPost <- precPrior + precData
    wPrior <- precPrior/precPost
    muPost <- wPrior*mu[j-1] + (1-wPrior)*mean(X)
    tau2Post <- 1/precPost
    mu[j] <- rnorm(1, mean = muPost, sd = sqrt(tau2Post))
    sigma2[j] <- rScaledInvChi2(1, df = nu0 + nObs, scale = (nu0*sigma2_0 + sum((X - mu[j])^2))/(nu0 + nObs))
    #Plug in Data
    gibbsDraws2[j,] = c(mu[j],sigma2[j])
  }
}
```

```{r, echo=FALSE}
set.seed(170121)
#Store Value
{
  gibbsDraws3 <- data.frame(matrix(0, ncol = 2 , nrow = nIter)) #storage
  #Start 
  sigma2 <- sigma2_0
  for(i in 1:1){
    precPrior <- 1/tau2Prior
    precData <- nObs/sigma2
    precPost <- precPrior + precData
    wPrior <- precPrior/precPost
    muPost <- wPrior*muPrior + (1-wPrior)*mean(X)
    tau2Post <- 1/precPost
    mu[i] <- rnorm(1, mean = muPost, sd = sqrt(tau2Post))
    sigma2[i] <- rScaledInvChi2(1, df = nu0 + nObs, scale = (nu0*sigma2_0 + sum((X - mu[i])^2))/(nu0 + nObs))
    #Plug in Data
    gibbsDraws3[i,] = c(mu[i],sigma2[i])
  }
  for (j in 2:nIter){
    precPrior <- 1/tau2Prior
    precData <- nObs/sigma2
    precPost <- precPrior + precData
    wPrior <- precPrior/precPost
    muPost <- wPrior*mu[j-1] + (1-wPrior)*mean(X)
    tau2Post <- 1/precPost
    mu[j] <- rnorm(1, mean = muPost, sd = sqrt(tau2Post))
    sigma2[j] <- rScaledInvChi2(1, df = nu0 + nObs, scale = (nu0*sigma2_0 + sum((X - mu[j])^2))/(nu0 + nObs))
    #Plug in Data
    gibbsDraws3[j,] = c(mu[j],sigma2[j])
  }
}
```

```{r, echo=FALSE}
par(mfrow = c(2,3))
plot(gibbsDraws[,1], type = "l", xlab = "iteration",ylab = "Mu from Gibbs Sampling", col = prettyCol[2]);
plot(gibbsDraws2[,1], type = "l", xlab = "iteration",ylab = "Mu from Gibbs Sampling Chain2", col = prettyCol[2]);
plot(gibbsDraws3[,1], type = "l", xlab = "iteration",ylab = "Mu from Gibbs Sampling Chain3", col = prettyCol[2]);
plot(gibbsDraws[,2], type = "l", xlab = "iteration",ylab = "Sigma from Gibbs Sampling", col = prettyCol[8]);
plot(gibbsDraws2[,2], type = "l", xlab = "iteration",ylab = "Sigma from Gibbs Sampling Chain2", col = prettyCol[8]);
plot(gibbsDraws3[,2], type = "l", xlab = "iteration",ylab = "Sigma from Gibbs Sampling Chain3", col = prettyCol[8]);
par(mfcol=c(1,1))
```

Comparing with another 2 chains of iteration, we are certain that the values of sampled parameters ($\mu,\sigma$) converge.

(b) \emph{Mixture Normal Model}

We implement Gibbs Sampling for two-component mixture of normals model:
$$p(y_i|\mu,\sigma^2,\pi)= \pi N(y_i|\mu_1,\sigma_1^2) + (1-\pi) N(y_i|\mu_2,\sigma_2^2)$$
  where $\mu= (\mu_1,\mu_2)$ and $\sigma^2 = (\sigma_1^2,\sigma_2^2)$

We use the Gibbs sampling algorithm to analyze the daily precipitation data (with the same number of iteration). 

* The prior parameters are set as they were in (a) with additional dimension for the second component.


```{r, echo=FALSE}
  #Settings
  X = as.matrix(rainfall)
  nComp <- 2    # Number of mixture components
  nIter <- 1000 # Number of Gibbs sampling draws
  nObs <- length(X)
  # Prior options
  alpha <- 10*rep(1,nComp) # Dirichlet(alpha)
  muPrior <- rep(0,nComp) # Prior mean of mu
  tau2Prior <- rep(10,nComp) # Prior std of mu
  sigma2_0 <- rep(5,nComp) # s20 (best guess of sigma2)
  nu0 <- rep(2,nComp) # degrees of freedom for prior on sigma2
```

```{r, echo=FALSE, include=FALSE}
{
  # Plotting options
  plotFit <- TRUE
  lineColors <- c("blue", "green", "magenta", 'yellow')
  sleepTime <- 0.1 # Adding sleep time between iterations for plotting
}
{
  #Define function necessary for mixture 
}
####### Defining a function that simulates from a Dirichlet distribution
rDirichlet <- function(param){
  nCat <- length(param)
  piDraws <- matrix(NA,nCat,1)
  for (j in 1:nCat){
    piDraws[j] <- rgamma(1,param[j],1)
  }
  piDraws = piDraws/sum(piDraws) # Diving every column of piDraws by the sum of the elements in that column.
  return(piDraws)
}

# Simple function that converts between two different representations of the mixture allocation
S2alloc <- function(S){
  n <- dim(S)[1]
  alloc <- rep(0,n)
  for (i in 1:n){
    alloc[i] <- which(S[i,] == 1)
  }
  return(alloc)
}

```

```{r, echo=FALSE, include=FALSE}
#dont want to show the simulation in the final output
# Initial value for the MCMC
nObs <- length(X)
S <- t(rmultinom(nObs, size = 1 , prob = rep(1/nComp,nComp))) # nObs-by-nComp matrix with component allocations.
mu <- quantile(X, probs = seq(0,1,length = nComp))
sigma2 <- rep(var(X),nComp)
probObsInComp <- rep(NA, nComp)

# Setting up the plot
xGrid <- seq(min(X)-1*apply(X,2,sd),max(X)+1*apply(X,2,sd),length = 100)
xGridMin <- min(xGrid)
xGridMax <- max(xGrid)
mixDensMean <- rep(0,length(xGrid))
effIterCount <- 0
ylim <- c(0,2*max(hist(X)$density))

{
  #Start Iteration
  for (k in 1:nIter){
    message(paste('Iteration number:',k))
    alloc <- S2alloc(S) # Just a function that converts between different representations of the group allocations
    nAlloc <- colSums(S)
    print(nAlloc)
    # Update components probabilities
    pi <- rDirichlet(alpha + nAlloc)
    
    #
    
    # Update mu's
    for (j in 1:nComp){
      precPrior <- 1/tau2Prior[j]
      precData <- nAlloc[j]/sigma2[j]
      precPost <- precPrior + precData
      wPrior <- precPrior/precPost
      muPost <- wPrior*muPrior + (1-wPrior)*mean(X[alloc == j])
      tau2Post <- 1/precPost
      mu[j] <- rnorm(1, mean = muPost, sd = sqrt(tau2Post))
    }
    
    # Update sigma2's
    for (j in 1:nComp){
      sigma2[j] <- rScaledInvChi2(1, df = nu0[j] + nAlloc[j], scale = (nu0[j]*sigma2_0[j] + sum((X[alloc == j] - mu[j])^2))/(nu0[j] + nAlloc[j]))
    }
    
    # Update allocation
    for (i in 1:nObs){
      for (j in 1:nComp){
        probObsInComp[j] <- pi[j]*dnorm(X[i], mean = mu[j], sd = sqrt(sigma2[j]))
      }
      S[i,] <- t(rmultinom(1, size = 1 , prob = probObsInComp/sum(probObsInComp)))
    }
    # Printing the fitted density against data histogram
    if (plotFit && (k%%1 ==0)){
      effIterCount <- effIterCount + 1
      hist(X, breaks = 20, freq = FALSE, xlim = c(xGridMin,xGridMax), main = paste("Iteration number",k), ylim = ylim)
      mixDens <- rep(0,length(xGrid))
      components <- c()
      for (j in 1:nComp){
        compDens <- dnorm(xGrid,mu[j],sd = sqrt(sigma2[j]))
        mixDens <- mixDens + pi[j]*compDens
        lines(xGrid, compDens, type = "l", lwd = 2, col = lineColors[j])
        components[j] <- paste("Component ",j)
      }
      mixDensMean <- ((effIterCount-1)*mixDensMean + mixDens)/effIterCount
    }
      
      lines(xGrid, mixDens, type = "l", lty = 2, lwd = 3, col = 'red')
      legend("topleft", box.lty = 1, legend = c("Data histogram",components, 'Mixture'), 
             col = c("black",lineColors[1:nComp], 'red'), lwd = 2)
      Sys.sleep(sleepTime)
  }
  }
```


(c) \emph{Graphical Comparison}
The diagram below illustrating 1) A histogram of the data and the posterior means of the parameters $\mu$ from 2) the normal density $N (\hat{\mu},\hat{\sigma}^2)$ in (a) and 3) the mixture of normal density $p(y_i|\mu,\sigma^2,\pi)$ in (b)

```{r, echo=FALSE}
hist(X, breaks = 20, freq = FALSE, xlim = c(xGridMin,xGridMax), main = "Final fitted density")
lines(xGrid, mixDensMean, type = "l", lwd = 2, lty = 4, col = "red")
lines(xGrid, dnorm(xGrid, mean = mean(gibbsDraws[,1]), sd = sd(gibbsDraws[,2])), type = "l", lwd = 2, col = "blue")
legend("topright", box.lty = 1, legend = c("Data histogram","Mixture density from (b)","Normal density from (a)"), col=c("black","red","blue"), lwd = 2)
```

\section{Question 2}
(a) We first fit the Poisson regression using MLE.
```{r echo=FALSE, results=TRUE}
ebayBid <- read.table("https://raw.githubusercontent.com/mattiasvillani/BayesLearnCourse/master/Labs/eBayNumberOfBidderData.dat", header = T)
glmModel <- glm(nBids~0+., data = ebayBid, family=poisson)
summary(glmModel)
```
From MLE, we found that ```const```,```verifyID```,```sealed```,```majBlem```,```LogBook```, and ```MinBidshare``` are significant.

(b) We now approximate the posterior distribution of $\beta$ with a multivariate normal distribution
$$ \beta|y, X \sim N (\tilde{\beta}, J_y^{-1}(\tilde{\beta}))$$
```{r echo=FALSE, results=TRUE}
y <- as.vector(ebayBid[,1]); X <- as.matrix(ebayBid[,2:10]);
covNames <- names(ebayBid)[2:length(names(ebayBid))]; nPara <- dim(X)[2];
# Setting up the prior
mu_0 <- as.vector(rep(0,nPara)); sigma_0 <- 100*solve(t(X)%*%X);

# Coding up the log posterior function
PostPoisson <- function(betaVect,y,X,mu_0,sigma_0,...){
  nPara <- length(betaVect);
  logPrior <- dmvnorm(x = betaVect, mean = matrix(0,nPara,1), sigma = sigma_0, log=TRUE);
  xB <- X%*%t(t(betaVect))  
  logLik <- sum(y*X%*%t(t(betaVect)) - exp(X%*%t(t(betaVect))) - log(factorial(y)));
  return(logLik + logPrior)
  }

initVal <- as.vector(glmModel$coefficients); 
OptimResults<-optim(initVal,PostPoisson,gr=NULL,y,X,mu_0,sigma_0,
                    method=c("BFGS"), control=list(fnscale=-1),hessian=TRUE)
postMode = OptimResults$par
postCov = -solve(OptimResults$hessian) # inv(J) - Approx posterior covariance matrix
postStd <- sqrt(diag(postCov)) # Computing approximate stdev
names(postMode) <- covNames; names(postStd) <- covNames # Naming the coefficient by covariates
colnames(postCov) <- covNames; rownames(postCov) <- covNames  # Naming the coefficient by covariates
```
Using ```optim.R```, the prior given in the question and the MLE results as initial values, $\tilde{\beta}$ are estimated as

```{r, echo=FALSE}
print(round(postMode, digits = 3))
```
and $J_y^{-1}(\tilde{\beta})$ are estimated as
```{r, echo=FALSE}
print(round(postCov, digits = 5))
```
(c) We are simulating the actual posterior of $\beta$ using the Metropolis Algorithm. We set the number of iteration as $10,000$ in burn-in period and hence do another $10,000$ iteration for beta sampling. To see if the sampled parameters actually converge, we run another 2 chains to make sure that they converge to the same distribution.

```{r, echo=FALSE}

LogPostFun <- function(theta,...){
  nPara <- length(theta)
  logPrior <- dmvnorm(x = theta, mean = matrix(0,nPara,1), sigma = sigma_0, log=TRUE);
  xB <- X%*%t(t(theta))  
  logLik <- sum(y*X%*%t(t(theta)) - exp(X%*%t(t(theta))) - log(factorial(y)));
  return(logLik + logPrior)
}

RWMSampler <- function(theta0, sigma, c, nIter,LogPostFun,...){
  # initialise the chain
  chain = matrix(NA, nrow=nIter, ncol=length(theta0)); colnames(chain) <- covNames
  chain[1,] = theta0
  accept <- rep(0, nIter)
  # 
  for (i in 2:nIter){
    theta_p <- mvrnorm(n=1, mu = chain[i-1,], Sigma = c*sigma)
    alpha <-  LogPostFun(theta_p,y,X,mu_0,sigma_0) -  LogPostFun(chain[i-1,],y,X,mu_0,sigma_0)
    if (log(runif(1)) < alpha){
      chain[i,] = theta_p
      accept[i] = 1
    }
    else{
      chain[i,] = chain[i-1,]
    }
  }
  cat("Acceptance rate:", sum(accept)/length(accept))
  return(chain)
}
```


```{r, echo=FALSE, include=FALSE}
set.seed(100121)
burnin <- RWMSampler(theta0 = as.vector(rep(0, nPara)), sigma = postCov, c = 0.6,
                    nIter = 10000, LogPostFun = PostPoisson)
set.seed(150121)
burnin2 <- RWMSampler(theta0 = as.vector(rep(0, nPara)), sigma = postCov, c = 0.6,
                    nIter = 10000, LogPostFun = PostPoisson)
set.seed(170121)
burnin3 <- RWMSampler(theta0 = as.vector(rep(0, nPara)), sigma = postCov, c = 0.6,
                    nIter = 10000, LogPostFun = PostPoisson)
```


```{r, echo=FALSE, include=FALSE}
estimation <- RWMSampler(theta0 = as.vector(colMeans(burnin)), sigma = postCov, c = 0.6,
                     nIter = 10000, LogPostFun = PostPoisson)
MHres <- colMeans(estimation)
```
The following plots show the trace plot of MCMC for each parameters during the burn-in period.
```{r, echo=FALSE}
par(mfrow = c(3,3))
plot(burnin[,1], type = "l", col = prettyCol[1], main = "Trace plot for Const Chain 1")
plot(burnin2[,1], type = "l", col = prettyCol[1], main = "Trace plot for Const Chain 2")
plot(burnin3[,1], type = "l", col = prettyCol[1], main = "Trace plot for Const Chain 3")
plot(burnin[,2], type = "l", col = prettyCol[2], main = "Trace plot for PowerSeller Chain 1")
plot(burnin2[,2], type = "l", col = prettyCol[2], main = "Trace plot for PowerSeller Chain 2")
plot(burnin3[,2], type = "l", col = prettyCol[2], main = "Trace plot for PowerSeller Chain 3")
plot(burnin[,3], type = "l", col = prettyCol[3], main = "Trace plot for VerifyID Chain 1")
plot(burnin2[,3], type = "l", col = prettyCol[3], main = "Trace plot for VerifyID Chain 2")
plot(burnin3[,3], type = "l", col = prettyCol[3], main = "Trace plot for VerifyID Chain 3")
plot(burnin[,4], type = "l", col = prettyCol[4], main = "Trace plot for Sealed Chain 1")
plot(burnin2[,4], type = "l", col = prettyCol[4], main = "Trace plot for Sealed Chain 2")
plot(burnin3[,4], type = "l", col = prettyCol[4], main = "Trace plot for Sealed Chain 3")
plot(burnin[,5], type = "l", col = prettyCol[5], main = "Trace plot for Minblem Chain 1")
plot(burnin2[,5], type = "l", col = prettyCol[5], main = "Trace plot for Minblem Chain 2")
plot(burnin3[,5], type = "l", col = prettyCol[5], main = "Trace plot for Minblem Chain 3")
plot(burnin[,6], type = "l", col = prettyCol[6], main = "Trace plot for MajBlem Chain 1")
plot(burnin2[,6], type = "l", col = prettyCol[6], main = "Trace plot for MajBlem Chain 2")
plot(burnin3[,6], type = "l", col = prettyCol[6], main = "Trace plot for MajBlem Chain 3")
plot(burnin[,7], type = "l", col = prettyCol[7], main = "Trace plot for LargNeg Chain 1")
plot(burnin2[,7], type = "l", col = prettyCol[7], main = "Trace plot for LargNeg Chain 2")
plot(burnin3[,7], type = "l", col = prettyCol[7], main = "Trace plot for LargNeg Chain 3")
plot(burnin[,8], type = "l", col = prettyCol[8], main = "Trace plot for LogBook Chain 1")
plot(burnin2[,8], type = "l", col = prettyCol[8], main = "Trace plot for LogBook Chain 2")
plot(burnin3[,8], type = "l", col = prettyCol[8], main = "Trace plot for LogBook Chain 3")
plot(burnin[,9], type = "l", col = prettyCol[9], main = "Trace plot for MinBidShare Chain 1")
plot(burnin2[,9], type = "l", col = prettyCol[9], main = "Trace plot for MinBidShare Chain 2")
plot(burnin3[,9], type = "l", col = prettyCol[9], main = "Trace plot for MinBidShare Chain 3")
```

These traceplots show the convergence of MCMC for all parameters as they move around the certain range although though the sampling values of each parameter are different in the beginning (before 2000th iteration) in different chain. The mean posteriors of each $\beta$ are listed below. Please note that these mean posteriors are sampled after the burn-in period as shown earlier.
```{r, echo=FALSE}
print(round(MHres, digits = 3))
```

We then compared the posteriors of each parameter from our simulated MCMC and numerical approximation by plotting the histogram of the simulated draws after the converged (after 3000th iteration) and draw the normal density curve with mean and standard deviation from (b). The results from (b) and (C) are consistent.


```{r, fig.show="hold", out.width="50%", echo=FALSE}
#select after burn-in
res<-tail(burnin, -3000)
for(i in 2:9){
  hist(res[,i], main=c(names(postMode)[i]),probability = TRUE,xlab='posterior')
  curve(dnorm(x,mean=postMode[i],sd=postStd[i]),type="l",ylab="density",add = TRUE, col = "blue")
  legend("topright", box.lty = 1, legend = c("MCMC Posterior histogram","Normal density from (b)"), col=c("black","blue"), lwd = 2)
}
```


\section{Question 3}

(a) We created a simulation function with respect to AR(1)-process
$$x_t = \mu + \phi(x_{t-1} - \mu) + \epsilon_t ,  \epsilon_t \overset{\text{iid}}{\sim} N(0,\sigma^2) $$

```{r, echo=FALSE}
SimAR1<- function(Time,mu,phi,epsilonmu,epsilonsd2){
  XSim <- data.frame(matrix(0, ncol = 3 , nrow = Time)) #storage
  for(i in 1:1){
    epsilon_t<-rnorm(1, mean=epsilonmu, sd=sqrt(epsilonsd2))
    x <- mu                 
    XSim[i,] = c(i,x,epsilon_t)
  }
  for(j in 2:Time){
    epsilon_t<-rnorm(1, mean=epsilonmu, sd=sqrt(epsilonsd2))
    x_lagged <- XSim$X2[j-1]
    x_t1_mu <- x_lagged - mu
    x <- mu+phi*(x_t1_mu)+epsilon_t              
    XSim[j,] = c(j,x,epsilon_t)
  }
  colnames(XSim) <- c("t", "x_t", "epsilon_t")
  return(XSim)
}
```

Then, we generated a number of time series data with different realizations for values of $\phi$ given $\mu = 10$ $\sigma^2 = 2$ and $T=200$
```{r, echo=FALSE}
  set.seed(100121)
  Sim_05<-SimAR1(Time=200, mu=10, phi=0.5, epsilonmu=0, epsilonsd2=2)
  Sim_08<-SimAR1(Time=200, mu=10, phi=0.8, epsilonmu=0, epsilonsd2=2)
  Sim_04<-SimAR1(Time=200, mu=10, phi=-0.4, epsilonmu=0, epsilonsd2=2)
  Sim_09<-SimAR1(Time=200, mu=10, phi=-0.2, epsilonmu=0, epsilonsd2=2)
```

```{r,  fig.show="hold", out.width="50%", echo=FALSE}
library(ggplot2)

ggplot(data = Sim_05, aes(x = t, y = x_t)) + 
     geom_line(color = "#00AFBB", size = 1)+ggtitle("Simulated Time Series for phi = 0.5")
ggplot(data = Sim_08, aes(x = t, y = x_t)) + 
     geom_line(color = "#CC6666", size = 1)+ggtitle("Simulated Time Series for phi = 0.8")
ggplot(data = Sim_04, aes(x = t, y = x_t)) + 
     geom_line(color = "#CC99FF", size = 1)+ggtitle("Simulated Time Series for phi = -0.4")
ggplot(data = Sim_04, aes(x = t, y = x_t)) + 
     geom_line(color = "#FF3399", size = 1)+ggtitle("Simulated Time Series for phi = -0.2")
```
The smaller the $\phi$ the fluctuated the $x_t$ 

(b) We used our simulation function to generate $x_{1:t}$ with $\phi = 0.3$ and $y_{1:t}$ with $\phi = 95$
```{r, echo=FALSE}
set.seed(100121)
  x_t<-SimAR1(Time=200, mu=10, phi=0.3, epsilonmu=0, epsilonsd2=2)
  y_t<-SimAR1(Time=200, mu=10, phi=0.95, epsilonmu=0, epsilonsd2=2)
```

* We wrote the stan code that samples from the posterior of the three parameters ($\mu,\phi,\epsilon$) and set

$$\mu \sim N(\mu + \phi(x_{t-1} - \mu),\sigma^2)$$

* However we found that setting specific parameters priors will limit the MCMC results to the given priors. Hence, we did not specify any parameters prior which should lead to Stan default setting:
$$\theta \sim U(-\infty,\infty)$$

This is non-informative priors.

i) We implement our ```Stan``` code to our generated $x_t$ and $y_t$ setting number of iteration as $1,000$ (with warm up = $500$ and post-warmup draw = $500$)

```{r, echo=FALSE, include=FALSE}
set.seed(100121)
#Set directory for Stan function
library(rstan)

#Compile model
model3b<-stan_model("BayeML_Rmarkdown_3_3b.stan")
fit_x_t<-sampling(model3b,list(T = 200, y = x_t$x_t),iter=1000)
fit_y_t<-sampling(model3b,list(T = 200, y = y_t$x_t),iter=1000)

```
The posterior means of the three inferred parameters of \textbf{$x_t$}

```{r, echo=FALSE}
print(fit_x_t)
```
The posterior means of the three inferred parameters of \textbf{$y_t$}
```{r, echo=FALSE}
print(fit_y_t)
```

Without giving specific priors of parameters, it seems that we are able to estimate the true $\phi$ values of both $x_t$ and $y_t$. However, we only get the true value of $\mu$ of $x_t$ correctly. Our posterior of $\mu$ of $y_t$ is fluctuated and the mean is higher than the true value.

ii) We use traceplot to evaluate the convergence of the samplers and see the joint posterior of $\mu$ and $\phi$. 

```{r,fig.show="hold", out.width="50%", echo=FALSE}
  library(RColorBrewer)
  prettyCol = brewer.pal(10,"Paired")
posterior_x_t <- extract(fit_x_t)
plot(posterior_x_t$mu, type = "l",col = prettyCol[2])
plot(posterior_x_t$phi, type = "l",col = prettyCol[2])
posterior_y_t <- extract(fit_y_t)
plot(posterior_y_t$mu, type = "l",col = prettyCol[8])
plot(posterior_y_t$phi, type = "l",col = prettyCol[8])
```

* For $x_t$, the joint posterior of $\mu$ and $\phi$ seems to be around the certain range and likely to converge. This reflect the data stationary property of this generated series. 

* For $y_t$, the joint posterior of $\mu$ and $\phi$ seems to fluctuate a lot. Even though $\mu$ seems to converge after $500th$ iteration, $\phi$ does not seems to converge. This will lead to incorrect estimation of $y_t$ because we also need $\phi$ to estimate.

(c) We modify our ```Stan``` code by \emph{setting $x_t$ as another parameter following AR(1) process} and implement to $campy$ data to estimate the number of infections $c_t$ as follows:
$$c_t|x_t \sim Poisson(exp(x_t))$$
where $x_t$ is an AR(1) process as in (a).
* We started by setting the number of iteration as $10,000$ and then increased to $30,000$ (with warm up = $15,000$ and post-warmup draw = $15,000$) because the ```Stan``` warns of non-convergence in parameters.
* As we thought that setting priors of $\mu, \phi, \sigma$ could limit the posterior values to some extent, we decided to not specify any parameters prior which should lead to Stan default setting:
$$\theta \sim U(-\infty,\infty)$$

```{r,echo=FALSE}
  campy <- read.table("https://github.com/mattiasvillani/BayesLearnCourse/raw/master/Labs/campy.dat", 
                         header=TRUE)
```

```{r, echo=FALSE, include=FALSE}
#Compile model
model3c<-stan_model("BayeML_Rmarkdown_3_3c.stan")
fit_campy<-sampling(model3c,list(T = length(campy$c), y = campy$c),iter=30000)
#options(max.print=10000)
#print(fit_campy)
```
* Implementing modified ```Stan``` code, we obtained posterior of each $x_t$. Then we exponentiated posterior of each $x_t$ and plot posterior means and their 95% confidence interval along with the data.

```{r, echo=FALSE}
#Laborious way of creating plot of x
x_summary <- summary(fit_campy, pars = c("x"), probs = c(0.025, 0.975))$summary #summary of value of x with probabiliy 0.25 and 0.975
exp_x_mean <- exp(x_summary[,1]) #exponential of mean of X
exp_x_lower <- exp(x_summary[,4]) #exponential of mean of X
exp_x_upper <- exp(x_summary[,5]) #exponential of mean of X
c_x <- data.frame(rownames(campy),campy$c,exp_x_mean,exp_x_lower,exp_x_upper) #merge data to see the value
colnames(c_x) <- c("t", "actualCase", "Mean_posterior","Lower_posterior","Upper_posterior")

plot(c_x[,2],main = "Data and Posterior Plot Without Specific Prior Setting", xlab = "Time", ylab = "Cases", ylim = c(0,60), xlim=c(0,140), col = "darkgrey", cex.lab = 0.8);
lines(c_x[,3], col = prettyCol[4]); #median
lines(c_x[,4], col = prettyCol[2]); #lower
lines(c_x[,5], col = prettyCol[6]); #upper
legend("topright", box.lty = 1, c("Mean Posterior","Lower 2.5% CI","Upper 97.5% CI"), lwd = 1, cex = 0.8, col = c(prettyCol[4],prettyCol[2], prettyCol[6]))

```

(d) We modify our ```Stan``` code by adding prior of $\sigma^2$ as $$\sigma^2 \sim N(0,0.00000001)$$ 

```{r, echo=FALSE, include=FALSE}
#Compile model
model3d<-stan_model("BayeML_Rmarkdown_3_3d.stan")
fit_campy2<-sampling(model3d,list(T = length(campy$c), y = campy$c),iter=30000)
#options(max.print=10000)
#print(fit_campy2)
```

* Implementing modified ```Stan``` code, we obtained posterior of each $x_t$. Then we exponentiated posterior of each $x_t$ and plot posterior means and their 95% confidence interval along with the data.

```{r, echo=FALSE}
x2_summary <- summary(fit_campy2, pars = c("x"), probs = c(0.025, 0.975))$summary #summary of value of x with probabiliy 0.25 and 0.975
exp_x_mean <- exp(x2_summary[,1]) #exponential of mean of X
exp_x_lower <- exp(x2_summary[,4]) #exponential of mean of X
exp_x_upper <- exp(x2_summary[,5]) #exponential of mean of X
c_x2 <- data.frame(rownames(campy),campy$c,exp_x_mean,exp_x_lower,exp_x_upper) #merge data to see the value
colnames(c_x2) <- c("t", "actualCase", "Mean_posterior","Lower_posterior","Upper_posterior")

plot(c_x2[,2], main = "Data and Posterior Plot After Minimizing Prior of Sigma", xlab = "Time", ylab = "Cases", ylim = c(0,60), xlim=c(0,140), col = "darkgrey", cex.lab = 0.8);
lines(c_x2[,3], col = prettyCol[4]); #median
lines(c_x2[,4], col = prettyCol[2]); #lower
lines(c_x2[,5], col = prettyCol[6]); #upper
legend("topright", box.lty = 1, c("Mean Posterior","Lower 2.5% CI","Upper 97.5% CI"), lwd = 1, cex = 0.8, col = c(prettyCol[4],prettyCol[2], prettyCol[6]))
```

We found that the posterior means and the confidence intervals when $\sigma^2$ is small are different from (c). The graph may not show the big differences clearly as the number is not much different in the illustrated graph. In general, the line of posteriors in this case is smoother as it has less number of sharp spike compared to the former case with non-informative priors. Moreover, the absolute value of $exp(x_t)$ in (d) are smaller than $exp(x_t)$ in (c).