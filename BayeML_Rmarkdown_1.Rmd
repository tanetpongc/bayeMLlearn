---
title: "Exploring posterior distributions in one-parameter models by simulation and direct numerical evaluation"
output: pdf_document
author: Huong Nguyen and Tanetpong Choungprayoon
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
library(ggplot2)
library(LearnBayes)
library(mgcViz)
library(geoR)
library(bayestestR)
```
\section{Question 1}
(a) The columns in the graphs below represent the $\theta$ simulated from the posterior distribution with increasing number of draws, and the curves are the theoretical posterior distribution.
We observe graphically the larger the number of draws, the closer the simulated means and variances are to the theoretical posterior mean (0.67) and variance (0.009).

```{r echo=FALSE, fig.cap="Simulations from the posterior distribution with increasing number of draws "}
set.seed(270692)
par(mfrow = c(2,2))
hist(rbeta(10, 14, 8), xlab = "p", xlim = c(0.2,1), ylim = c(0,6), breaks=10, main = "nDraws = 10",cex.main = 0.7, freq = F)
curve(dbeta(x,14, 8), from=0, to=1,lty=1,lwd=1, add = TRUE)
hist(rbeta(100, 14, 8), xlab = "p",  xlim = c(0.2,1), ylim = c(0,6), breaks=10, main = "nDraws = 100", cex.main = 0.7,freq = F)
curve(dbeta(x,14, 8), from=0, to=1,lty=1,lwd=1, add = TRUE)
hist(rbeta(1000, 14, 8), xlab = "p",  xlim = c(0.2,1), ylim = c(0,6), breaks=10, main = "nDraws = 1000", cex.main = 0.7,freq = F)
curve(dbeta(x,14, 8), from=0, to=1,lty=1,lwd=1, add = TRUE)
hist(rbeta(1000, 14, 8), xlab = "p",  xlim = c(0.2,1), ylim = c(0,6), breaks=10, main = "nDraws = 10000", cex.main = 0.7,freq = F)
curve(dbeta(x,14, 8), from=0, to=1,lty=1,lwd=1, add = TRUE)
par("mfcol"=c(1, 1))
```

(b) The theoretical posterior probability $\Pr(\theta<0.4|y)$ is 0.0122911.
The simulated posterior probability $\Pr(\theta<0.4|y)$ with ```nDraws = 10000``` is 0.0126.
They are not exactly the same but very close.
```{r echo=FALSE, message=FALSE, warning=FALSE}
set.seed(2706)
true1b = pbeta(0.4, 14, 8)
drawn1b = sum((rbeta(10000, 14, 8)) <= 0.4)/10000
```

\newpage
(c) Plugging the simulated $\theta$ into the log-odds formula, we have the simulated $\phi$.
The posterior distribution of $\phi$ is plotted in the graph below.
The columns are simulated $\phi$ and the curve is the density function.
```{r echo=FALSE, out.width = '60%', fig.align = "center"}
logodds = log((rbeta(10000, 14, 8))/(1 - (rbeta(10000, 14, 8))))
hist(logodds, ylim = c(0,1.3),freq = F, breaks = 30, main = "(Simulated) Log-odds posterior distribution")
lines(density(logodds), lwd =2)
```

\section{Question 2}
(a) Based on the formula, the theoretical $\tau^2$ when $\mu = 3.5$ given the data collected can be calculated as
```{r echo=FALSE}
y = c(14,25,45,25,30,33,19,50,34,67)
n2 = 10; mu = 3.5
tau_squared = sum((log(y)-mu)^2)/n2; print(tau_squared)
```
We then draw from the $Inv - \chi^2(n, \tau^2)$ and plot them in the graph below.
The columns are simulated $\sigma^2$ with with ```nDraws = 10000``` and the curve is the theoretical posterior distribution.
```{r echo=FALSE, out.width = '60%', fig.align = "center"}
set.seed(2706)
nDraws_2 = 10000
Post_2_simulated = rinvchisq(nDraws_2, df = n2, tau_squared)
hist(Post_2_simulated, freq = F, xlim = c(0,1), ylim = c(0,5), breaks = 120, main = "Theoretical and simulated posterior distribution", xlab = "sigma^2")
curve(dinvchisq(x, df = n2, tau_squared), lwd =2, add = TRUE)
```
(b) Plugging the simulated $\sigma^2$ from (3a) to the formula $G = 2\Phi(\sigma/2) - 1$, we obtain the Gini coefficient for the current data set, which are plotted in the graph below.
```{r echo=FALSE, out.width = '80%', fig.align = "center"}
set.seed(2706)
z=sqrt(Post_2_simulated)/sqrt(2)
G = 2*(pnorm(z))-1
hist(G, freq = F, xlim = c(0,1), ylim = c(0,9), breaks = 30, main = "Posterior distribution of Gini coefficient", cex.main = 0.8)
```

(c) The 95% equal tail credible interval for G is [0.14, 0.39]. The Higest Posterior Density interval for G is [0.16, 0.40].
The two intervals are different (because the distribution is asymetric), but not substantially different (because the distribution is unimodal and not highly asymetric).

```{r echo=FALSE, message=FALSE}
lower = mean(G)-1.96*sd(G); upper = mean(G)+1.96*sd(G) #CI
HPD = hdi(G, ci = 0.95) #HPD
```

\section{Question 3}
(a) Since data points are i.i.d., the likelihood function is:
$$p(y|\mu , \kappa) = \prod_{i=1}^{10}\dfrac{\exp(\kappa \cdot \cos(y_i - \mu))}{2\pi I_0(\kappa)}
= \dfrac{\exp(\kappa \sum_{i=1}^{10} \cos(y_i - \mu))}{2^{10} \pi^{10} I_0(\kappa)^{10}}.
$$
Given the prior $$ \kappa \sim Exponential ( \lambda= 1), $$ the posterior:
$$p(\kappa|\mu , y) \propto \dfrac{\exp(\kappa \sum_{i=1}^{10} \cos(y_i - \mu)) \cdot \exp(-\kappa)}{ I_0(\kappa)^{10}}
$$
and
$$
p(\kappa|\mu , y)  = \dfrac{\exp(\kappa \sum_{i=1}^{10} \cos(y_i - \mu)-\kappa)/{ I_0(\kappa)^{10}} }{p(y)} , 
$$
where
$$ p(y) = \int_0^{+\infty} \dfrac{\exp(\kappa \sum_{i=1}^{10} \cos(y_i - \mu) - \kappa)}{ I_0(\kappa)^{10}} d\kappa
$$
\newpage
The posterior distribution is then plotted as follows.
```{r echo=FALSE, message=FALSE, out.width = '60%', fig.align = "center"}
y_3 = c(-2.44, 2.14, 2.54, 1.83, 2.02, 2.33, -2.79, 2.23, 2.07, 2.02)
mu_3 = 2.39
post_num = function(k) {exp(k*sum(cos(y_3-mu_3))-k)/(besselI(k,0,expon.scaled = F)^10)}
post_dem = integrate(post_num, lower = 0, upper = 99)
post_3 = function(k) {exp(k*sum(cos(y_3-mu_3))-k)/((besselI(k,0,expon.scaled = F)^10)*post_dem$value)}
curve(post_3, from = 0, to = 10, ylab = "Posterior density", xlab = "kappa")
```

(b) The (approximate) posterior mode of $\kappa$ is the $argmax (p(\kappa|\mu , y)) = 2.124762$. It's simple to calculate given the function is unimodal and smooth.

```{r echo=FALSE, message=FALSE, results='hide'}
optimize(post_3, c(0, 10), maximum = TRUE)
```